{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b04aa4",
   "metadata": {},
   "source": [
    "# Multi-Agent RAG Pipline\n",
    "RAG system contsin (3) major layers:\n",
    "1. Knowledge Layer (Vector Store)\n",
    "    - Where documents live, chunked and embdeed (for semantic searching)\n",
    "\n",
    "2. Retrieval Layer\n",
    "    - Takes a user query\n",
    "    - Finds relevant chunks\n",
    "    - Returns them as context\n",
    "\n",
    "3. Agent Layer\n",
    "    Team of LLM-powered agents that:\n",
    "    - Inerpret question\n",
    "    - Retrieve context\n",
    "    - Reason collaboratively\n",
    "    - Produce anser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f5e4f",
   "metadata": {},
   "source": [
    "## Setup: Import and environemnt\n",
    "To build a pipeline, we need:\n",
    "- Vector Store (Chroma DB)\n",
    "- Embedding Model (OpenAI)\n",
    "- Chunking logic (break long docs into retrivalbe pieces)\n",
    "- AutoGen Agents (to reason over retrieved context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378dd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb # vectore store backend\n",
    "from chromadb.config import Settings \n",
    "\n",
    "from openai import OpenAI   # for embeddings\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "import asyncio  # Autogen 0.4 uses aysnc execution\n",
    "import textwrap # clean chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddd404",
   "metadata": {},
   "source": [
    "### Load environment and initialize OpenAI + Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d174db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a359db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init openai client (used to generated embeddings)\n",
    "oa_client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Init ChromaDB (Persistent, Telemtry Off)\n",
    "# NOTE: Want local vector store that doesn't \"phone home\"\n",
    "client_settings = Settings(anonymized_telemetry=False)\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./chroma_db_multi_agent\",\n",
    "    settings=client_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45739d7",
   "metadata": {},
   "source": [
    "### Define OpenAI embedding function for Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967f5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'text-embedding-3-small'\n",
    "\n",
    "# NOTE: Wrap embedings in a class as chroma expected an object with __call__ method\n",
    "class OpenAIEmbeddingFunction:\n",
    "    def __init__(self, client, model: str):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input: list[str]) -> list[list[float]]:\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=input,\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        # Chroma requires this for conflict detection\n",
    "        return f\"openai={self.model}\"\n",
    "    \n",
    "embedding_fn = OpenAIEmbeddingFunction(oa_client, EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2663e9",
   "metadata": {},
   "source": [
    "### Create Chroma Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59760ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_client.delete_collection(\"rag_collection_multi_agent\")     # clean-up (if needed)\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"rag_collection_multi_agent\",\n",
    "    embedding_function=embedding_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}   # Use cosine as standard metric for semantic embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e0112",
   "metadata": {},
   "source": [
    "### Chunking Helper and Indexing Sample Content\n",
    "(LLM retreive better when documents broken into small, overlapping chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2e77de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50):\n",
    "    text = textwrap.dedent(text).strip()\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap # slide with overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785e49c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " ['AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. It supports tools, function calling, and orchestration patterns for LLM-based agents. Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an',\n",
       "  'a technique where a model retrieves relevant context from an external knowledge base (like a vector database) and uses it to ground its responses. By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve,'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index sample content\n",
    "kb_text = \"\"\" AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. \n",
    "It supports tools, function calling, and orchestration patterns for LLM-based agents. \n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an external \n",
    "knowledge base (like a vector database) and uses it to ground its responses. \n",
    "\n",
    "By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve, \n",
    "reason, and respond with up-to-date, domain-specific knowledge. \n",
    "\"\"\"\n",
    "\n",
    "# Chunk text\n",
    "chunks = chunk_text(kb_text, chunk_size=40, overlap=10)\n",
    "\n",
    "# Generate unique id for each chunk (chroma requries every document to have a unique ID)\n",
    "ids = [f\"chunk-{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# Insert chunked text into Chroma collection \n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=ids,\n",
    ")\n",
    "\n",
    "# ouput total number of chunks, preview first two chunks\n",
    "len(chunks), chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46ff03",
   "metadata": {},
   "source": [
    "(3,\n",
    " ['AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. It supports tools, function calling, and orchestration patterns for LLM-based agents. Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an',\n",
    "  'a technique where a model retrieves relevant context from an external knowledge base (like a vector database) and uses it to ground its responses. By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve,'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42bcd",
   "metadata": {},
   "source": [
    "## Build Multi-Agent Team\n",
    "This multi-agent team will have the following agents: researcher, writer, critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb668f",
   "metadata": {},
   "source": [
    "### Create Model Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0ddf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Did previously, restating for clarity\n",
    "model_client = OpenAIChatCompletionClient(model='gpt-4o', api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab623577",
   "metadata": {},
   "source": [
    "### Create the Agents\n",
    "Key things to note about agents\n",
    "- Like movie star actors, agents need clear-defined roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9aa4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = AssistantAgent(\n",
    "    name=\"researcher\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a researcher-focused agent. \"\n",
    "        \"Your job is to analyze the user's query, retrieve relevant context, \"\n",
    "        \"and propose a plan for answering the question. \"\n",
    "        \"Be thorough, analytical, and explicit about what information is needed.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "writer = AssistantAgent(\n",
    "    name=\"writer\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a writing-focused agent. \"\n",
    "        \"Your job is to synthesize a clear, accurate, grounded answer \"\n",
    "        \"using the retrieved context and the researcher's plan. \"\n",
    "        \"Write with clarity and precision\"\n",
    "    )\n",
    ")\n",
    "\n",
    "critic = AssistantAgent(\n",
    "    name=\"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a critical evaluator. \"\n",
    "        \"Your job is to review the writer's answer for correctness, \"\n",
    "        \"grounding in retrieved context, and clarity. \"\n",
    "        \"Suggest improvements when necessary\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca76d68",
   "metadata": {},
   "source": [
    "### Define Agent-to-Agent Messaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30626730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: Change in how communicate with agents\n",
    "'''\n",
    "Chat helper function\n",
    "\n",
    "Instead of having to run agents like:\n",
    "`await researcher.run(task=\"Here is my plan\", recipient=writer)`\n",
    "\n",
    "Can NOW just say:\n",
    "`await chat(researcher, writer, \"Here is my plan\")`\n",
    "'''\n",
    "async def chat(sender, receiver, message: str):\n",
    "    # Ask the sender agent to run a task\n",
    "    result = await sender.run(\n",
    "        task=message,   # what sender is saying\n",
    "        receiver=receiver   # who the sender is talkign to\n",
    "    )\n",
    "\n",
    "    # Return the result of that interaction\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a researcher-focused agent, my role is to analyze the user's query, retrieve relevant context and information, and propose a comprehensive plan for addressing their question or research needs.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "result = await researcher.run(task=\"Explain your role in one sentence.\")\n",
    "print(result.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c621b",
   "metadata": {},
   "source": [
    "As a researcher-focused agent, my role is to analyze the user's query, retrieve relevant context and information, and propose a comprehensive plan for addressing their question or research needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d18ed",
   "metadata": {},
   "source": [
    "## Buld Multi-AGent RAG Pipeline (Retrieval + Collaboration + Revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1926b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
