{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b2e1bf",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "Run locally using models downloaded to machine and local compute resources.  \n",
    "NOTE: ollama server needs to be running, sadly on my Mac most models locks up my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6415a9a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Assume Ollama server is running locally on port 11434\u001b[39;00m\n\u001b[32m      6\u001b[39m ollama_model_client = OllamaChatCompletionClient(model=\u001b[33m\"\u001b[39m\u001b[33mllama2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m ollama_model_client.create([UserMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is the captital of France?\u001b[39m\u001b[33m\"\u001b[39m, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ollama_model_client.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/04_Projects/Autogen_Lab/autogen_venv/lib/python3.14/site-packages/autogen_ext/models/ollama/_ollama_client.py:646\u001b[39m, in \u001b[36mBaseOllamaChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m result: ChatResponse = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    648\u001b[39m usage = RequestUsage(\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# TODO backup token counting\u001b[39;00m\n\u001b[32m    650\u001b[39m     prompt_tokens=result.prompt_eval_count \u001b[38;5;28;01mif\u001b[39;00m result.prompt_eval_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    651\u001b[39m     completion_tokens=(result.eval_count \u001b[38;5;28;01mif\u001b[39;00m result.eval_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m),\n\u001b[32m    652\u001b[39m )\n\u001b[32m    654\u001b[39m logger.info(\n\u001b[32m    655\u001b[39m     LLMCallEvent(\n\u001b[32m    656\u001b[39m         messages=[m.model_dump() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m create_params.messages],\n\u001b[32m   (...)\u001b[39m\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/04_Projects/Autogen_Lab/autogen_venv/lib/python3.14/site-packages/ollama/_client.py:983\u001b[39m, in \u001b[36mAsyncClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, logprobs, top_logprobs, format, options, keep_alive)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    936\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    947\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    948\u001b[39m ) -> Union[ChatResponse, AsyncIterator[ChatResponse]]:\n\u001b[32m    949\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    950\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    951\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    980\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    984\u001b[39m     ChatResponse,\n\u001b[32m    985\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    986\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    987\u001b[39m     json=ChatRequest(\n\u001b[32m    988\u001b[39m       model=model,\n\u001b[32m    989\u001b[39m       messages=\u001b[38;5;28mlist\u001b[39m(_copy_messages(messages)),\n\u001b[32m    990\u001b[39m       tools=\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[32m    991\u001b[39m       stream=stream,\n\u001b[32m    992\u001b[39m       think=think,\n\u001b[32m    993\u001b[39m       logprobs=logprobs,\n\u001b[32m    994\u001b[39m       top_logprobs=top_logprobs,\n\u001b[32m    995\u001b[39m       \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    996\u001b[39m       options=options,\n\u001b[32m    997\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    998\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    999\u001b[39m     stream=stream,\n\u001b[32m   1000\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/04_Projects/Autogen_Lab/autogen_venv/lib/python3.14/site-packages/ollama/_client.py:767\u001b[39m, in \u001b[36mAsyncClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    765\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_raw(*args, **kwargs)).json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/04_Projects/Autogen_Lab/autogen_venv/lib/python3.14/site-packages/ollama/_client.py:713\u001b[39m, in \u001b[36mAsyncClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    711\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assume Ollama server is running locally on port 11434\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama2\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"What is the captital of France?\", source=\"user\")])\n",
    "print(response)\n",
    "await ollama_model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock web search code\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Find information on the web\"\"\"\n",
    "    return \"The Labrador Retriever or simply Labrador is a British breed of retreiver gun dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agent\n",
    "agent = AssistantAgent(\n",
    "    name='assistant',\n",
    "    model_client=ollama_model_client,  # brain\n",
    "    system_message='You are a helpful assistant',\n",
    "    description='ollama test'    # not requried\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: don't run on Mac as weill crash \n",
    "#result = await agent.run(task='Find information about Labrador Retriever')\n",
    "#print(result.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31994a4",
   "metadata": {},
   "source": [
    "# Open Router\n",
    "Way to run LLMs (some of them free) via using cloud resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3ef48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "import os\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "open_router_api_key = os.environ.get('OPENROUTER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8b403b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briannabesinaiz/Documents/04_Projects/Autogen_Lab/autogen_venv/lib/python3.14/site-packages/autogen_ext/models/openai/_openai_client.py:466: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "open_router_model_client = OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"deepseek/deepseek-r1-0528:free\",\n",
    "    api_key = open_router_api_key,\n",
    "    model_info={\n",
    "        \"family\":'deepseek',\n",
    "        \"vision\":True,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\":False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92bbb508",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_agent1 = AssistantAgent(\n",
    "    name = 'helpful_agent',\n",
    "    model_client= open_router_model_client,\n",
    "    system_message='You are a helpful assistant'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8027d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Information about Labrador Retrievers:**  \n",
      "1. **Origin:** Labrador Retrievers originated from Newfoundland, Canada, in the 1800s, where they were initially used as fishing helpers.  \n",
      "2. **Traits:**  \n",
      "   - Temperament: Friendly, outgoing, loyal, and highly intelligent.  \n",
      "   -Áª¥ÂüÉÁä¨ÔºåÊãâÂ∏ÉÊãâÂ§öÂØªÂõûÁä¨  \n",
      "   - Size: 22.5-24.5 inches (male), 21.5-23.5 inches (female); weight 65-80 lbs (male), 55-70 lbs (female).  \n",
      "   - Coat: Short, dense, and waterproof; colors include black, yellow, and chocolate.  \n",
      "3. **Roles:** Widely used as guide dogs, search-and-rescue assistants, therapy dogs, and beloved family pets due to their trainability and gentle nature.  \n",
      "4. **Lifespan:** Typically 10‚Äì12 years.  \n",
      "5. **Care Needs:** Plays fetch  \n",
      "   - Exercise: Requires daily vigorous exercise (e.g., 1‚Äì2 hours) to prevent boredom.  \n",
      "   - Diet: Monitor food intake‚Äîprone to obesity.  \n",
      "   - Health Risks: Hip/elbow dysplasia, ear infections, and certain hereditary conditions.  \n",
      "\n",
      "---\n",
      "\n",
      "**Who I am:** üòä  \n",
      "I'm an AI assistant here to provide accurate, helpful information responsibly. My goal is to answer your questions clearly and safely! If you have more queries‚Äîabout pets or anything else‚Äîjust ask. üêæ\"\n"
     ]
    }
   ],
   "source": [
    "result = await assistant_agent1.run(task='Who are you?')\n",
    "print(result.messages[-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
