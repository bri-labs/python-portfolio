{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b04aa4",
   "metadata": {},
   "source": [
    "# Multi-Agent RAG Pipline\n",
    "RAG system contsin (3) major layers:\n",
    "1. Knowledge Layer (Vector Store)\n",
    "    - Where documents live, chunked and embdeed (for semantic searching)\n",
    "\n",
    "2. Retrieval Layer\n",
    "    - Takes a user query\n",
    "    - Finds relevant chunks\n",
    "    - Returns them as context\n",
    "\n",
    "3. Agent Layer\n",
    "    Team of LLM-powered agents that:\n",
    "    - Inerpret question\n",
    "    - Retrieve context\n",
    "    - Reason collaboratively\n",
    "    - Produce anser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f5e4f",
   "metadata": {},
   "source": [
    "## Setup: Import and environemnt\n",
    "To build a pipeline, we need:\n",
    "- Vector Store (Chroma DB)\n",
    "- Embedding Model (OpenAI)\n",
    "- Chunking logic (break long docs into retrivalbe pieces)\n",
    "- AutoGen Agents (to reason over retrieved context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378dd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb # vectore store backend\n",
    "from chromadb.config import Settings \n",
    "\n",
    "from openai import OpenAI   # for embeddings\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "import asyncio  # Autogen 0.4 uses aysnc execution\n",
    "import textwrap # clean chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ddd404",
   "metadata": {},
   "source": [
    "### Load environment and initialize OpenAI + Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d174db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a359db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init openai client (used to generated embeddings)\n",
    "oa_client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Init ChromaDB (Persistent, Telemtry Off)\n",
    "# NOTE: Want local vector store that doesn't \"phone home\"\n",
    "client_settings = Settings(anonymized_telemetry=False)\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./chroma_db_multi_agent\",\n",
    "    settings=client_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45739d7",
   "metadata": {},
   "source": [
    "### Define OpenAI embedding function for Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967f5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'text-embedding-3-small'\n",
    "\n",
    "# NOTE: Wrap embedings in a class as chroma expected an object with __call__ method\n",
    "class OpenAIEmbeddingFunction:\n",
    "    def __init__(self, client, model: str):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input: list[str]) -> list[list[float]]:\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=input,\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    \n",
    "    # Used for embedding quereis when calling colleciton.query()\n",
    "    def embed_query(self, input: list[str]) -> list[list[float]]:\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.model,\n",
    "            input=input\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        # Chroma requires this for conflict detection\n",
    "        return f\"openai={self.model}\"\n",
    "    \n",
    "embedding_fn = OpenAIEmbeddingFunction(oa_client, EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2663e9",
   "metadata": {},
   "source": [
    "### Create Chroma Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59760ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma_client.delete_collection(\"rag_collection_multi_agent\")     # clean-up (if needed)\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"rag_collection_multi_agent\",\n",
    "    embedding_function=embedding_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}   # Use cosine as standard metric for semantic embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e0112",
   "metadata": {},
   "source": [
    "### Chunking Helper and Indexing Sample Content\n",
    "(LLM retreive better when documents broken into small, overlapping chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e77de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50):\n",
    "    text = textwrap.dedent(text).strip()\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap # slide with overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785e49c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " ['AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. It supports tools, function calling, and orchestration patterns for LLM-based agents. Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an',\n",
       "  'a technique where a model retrieves relevant context from an external knowledge base (like a vector database) and uses it to ground its responses. By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve,'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index sample content\n",
    "kb_text = \"\"\" AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. \n",
    "It supports tools, function calling, and orchestration patterns for LLM-based agents. \n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an external \n",
    "knowledge base (like a vector database) and uses it to ground its responses. \n",
    "\n",
    "By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve, \n",
    "reason, and respond with up-to-date, domain-specific knowledge. \n",
    "\"\"\"\n",
    "\n",
    "# Chunk text\n",
    "chunks = chunk_text(kb_text, chunk_size=40, overlap=10)\n",
    "\n",
    "# Generate unique id for each chunk (chroma requries every document to have a unique ID)\n",
    "ids = [f\"chunk-{i}\" for i in range(len(chunks))]\n",
    "\n",
    "# Insert chunked text into Chroma collection \n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    ids=ids,\n",
    ")\n",
    "\n",
    "# ouput total number of chunks, preview first two chunks\n",
    "len(chunks), chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46ff03",
   "metadata": {},
   "source": [
    "(3,\n",
    " ['AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. It supports tools, function calling, and orchestration patterns for LLM-based agents. Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an',\n",
    "  'a technique where a model retrieves relevant context from an external knowledge base (like a vector database) and uses it to ground its responses. By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve,'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42bcd",
   "metadata": {},
   "source": [
    "## Build Multi-Agent Team\n",
    "This multi-agent team will have the following agents: researcher, writer, critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb668f",
   "metadata": {},
   "source": [
    "### Create Model Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0ddf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Did previously, restating for clarity\n",
    "model_client = OpenAIChatCompletionClient(model='gpt-4o', api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab623577",
   "metadata": {},
   "source": [
    "### Create the Agents\n",
    "Key things to note about agents\n",
    "- Like movie star actors, agents need clear-defined roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "writer = AssistantAgent(\n",
    "    name=\"writer\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a writing-focused agent. \"\n",
    "        \"Your job is to synthesize a clear, accurate, grounded answer \"\n",
    "        \"using the retrieved context and the researcher's plan. \"\n",
    "        \"Write with clarity and precision\"\n",
    "    )\n",
    ")\n",
    "\n",
    "critic = AssistantAgent(\n",
    "    name=\"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are a critical evaluator. \"\n",
    "        \"Your job is to review the writer's answer for correctness, \"\n",
    "        \"grounding in retrieved context, and clarity. \"\n",
    "        \"Suggest improvements when necessary\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca76d68",
   "metadata": {},
   "source": [
    "### Define Agent-to-Agent Messaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30626730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: Change in how communicate with agents\n",
    "'''\n",
    "Chat helper function\n",
    "\n",
    "Instead of having to run agents like:\n",
    "`await researcher.run(task=\"Here is my plan\", recipient=writer)`\n",
    "\n",
    "Can NOW just say:\n",
    "`await chat(researcher, writer, \"Here is my plan\")`\n",
    "'''\n",
    "async def chat(sender, receiver, message: str):\n",
    "    # Ask the sender agent to run a task\n",
    "    result = await sender.run(\n",
    "        task=message,   # what sender is saying\n",
    "        receiver=receiver   # who the sender is talkign to\n",
    "    )\n",
    "\n",
    "    # Return the result of that interaction\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51e0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My role is to analyze users' queries, gather relevant context, and propose a comprehensive plan for answering their questions with precision and clarity.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "result = await researcher.run(task=\"Explain your role in one sentence.\")\n",
    "print(result.messages[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c621b",
   "metadata": {},
   "source": [
    "As a researcher-focused agent, my role is to analyze the user's query, retrieve relevant context and information, and propose a comprehensive plan for addressing their question or research needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b05ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_stream(sender, receiver, message: str):\n",
    "    \"\"\"\n",
    "    Stream messages from one agent to another.\n",
    "    Yields each from as it arrives, and returns the final result.\n",
    "    \"\"\"\n",
    "    final_result = None\n",
    "\n",
    "    async for frame in sender.run_stream(\n",
    "        task=message,\n",
    "        recipient=receiver\n",
    "        ):\n",
    "        # Each frame is a dict with role + content\n",
    "        role = frame.get(\"role\", \"assistant\")\n",
    "        content = frame.get(\"content\", \"\")\n",
    "\n",
    "        # Display the streamed content\n",
    "        print(f\"[{sender.name} -> {receiver.name}] {role}: {content}\")\n",
    "\n",
    "        # Final frame contains full result object\n",
    "        if frame.get(\"event\") == \"on_complete\":\n",
    "            final_result = frame[\"result\"]\n",
    "\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d18ed",
   "metadata": {},
   "source": [
    "## Buld Multi-AGent RAG Pipeline (Retrieval + Collaboration + Revision)\n",
    "In the previous single RAG system, it simply retreived the context and answered questions\n",
    "In the multi-agent RAG pipeline, it does the following:\n",
    "1. Retrival - Find relevant chunks from vector store\n",
    "2. Researcher - Interprets question, analyzes retrieved context, proposes a plan\n",
    "3. Writer - Synthesizes grounded answer using plan + retrieved context\n",
    "4. Critic - Evaluates answer for correctness, grounding, clarity, hallucinations\n",
    "5. Writer (revision) - Improves answer based on critic's feeback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff73a66",
   "metadata": {},
   "source": [
    "### Build a Retrieval Helper for the Pipeline\n",
    "*Similar to notebook #1, but return BOTH chunks and metadata so researcher can reason about them*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1926b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrival Helper Function\n",
    "def retrieve_context(query: str, k: int = 4):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most relevant chunks from ChromaDB. \n",
    "    Returns both the chunks and their IDs for traceability.\n",
    "    \"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k\n",
    "    )\n",
    "\n",
    "    chunks = results[\"documents\"][0]\n",
    "    ids = results[\"ids\"][0]\n",
    "\n",
    "    print(chunks)\n",
    "    print(ids)\n",
    "\n",
    "    return chunks, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5541e7",
   "metadata": {},
   "source": [
    "### Multi-Agent RAG Pipeline Orchestrator\n",
    "Function coordinates entire multi-agent workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def multi_agent_rag(query: str):\n",
    "    \"\"\"\n",
    "    Full multi-agent RAG workflow:\n",
    "    1. Retrieve context\n",
    "    2. Researcher analyzes question + context\n",
    "    3. Writer drafts answer\n",
    "    4. Critic reviews answer\n",
    "    5. Writer revises\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: For the prompts, they are formated \n",
    "\n",
    "    # Step 1: Retrieve context\n",
    "    chunk, ids = retrieve_context(query)\n",
    "    context_block = \"\\n\\n\".join(\n",
    "        f\"[{ids[i]}] {chunks[i]}\" for i in range(len(chunks))\n",
    "    )\n",
    "\n",
    "    # Step 2: Researcher analyzes\n",
    "    researcher_prompt = f\"\"\"\n",
    "    User query:\n",
    "    {query}\n",
    "\n",
    "    Retrieved context:\n",
    "    {context_block}\n",
    "\n",
    "    Your task:\n",
    "    - Analyze the question\n",
    "    - Identify which chunks are relevant\n",
    "    - Propose a plan for how the writer should answer\n",
    "    \"\"\"\n",
    "\n",
    "    researcher_result = await researcher.run(task=researcher_prompt)\n",
    "    researcher_plan = researcher_result.messages[-1].content\n",
    "\n",
    "    # Step 3: Writer drafts answer\n",
    "    writer_prompt = f\"\"\"\n",
    "    User query:\n",
    "    {query}\n",
    "\n",
    "    Retrieved contex:\n",
    "    {context_block}\n",
    "\n",
    "    Researchers' plan:\n",
    "    {researcher_plan}\n",
    "\n",
    "    Your task:\n",
    "    - Write a clear, grounded answer\n",
    "    - Use the retrieved context explicitly\n",
    "    \"\"\"\n",
    "\n",
    "    writer_result = await writer.run(task=writer_prompt)\n",
    "    draft_answer = writer_result.messages[-1].content\n",
    "\n",
    "    # Step 4: Critic reviews\n",
    "    critic_prompt = f\"\"\"\n",
    "    Draft asnwer:\n",
    "    {draft_answer}\n",
    "\n",
    "    Retrieved context:\n",
    "    {context_block}\n",
    "\n",
    "    Your task:\n",
    "    - Evaluate correctness\n",
    "    - Check grounding in context\n",
    "    - Suggest improvements\n",
    "    \"\"\"\n",
    "\n",
    "    critic_result = await critic.run(task=critic_prompt)\n",
    "    critique = critic_result.messages[-1].content\n",
    "\n",
    "    # Step 5: Writer revises\n",
    "    revision_prompt = f\"\"\"\n",
    "    Original draft:\n",
    "    {draft_answer}\n",
    "\n",
    "    Critic's feedback:\n",
    "    {critique}\n",
    "\n",
    "    Your task:\n",
    "    - Produce a revised, improved final answer\n",
    "    \"\"\"\n",
    "\n",
    "    final_result = await writer.run(task=revision_prompt)\n",
    "    final_answer = final_result.messages[-1].content\n",
    "\n",
    "    return {\n",
    "        \"context\": context_block,\n",
    "        \"plan\": researcher_plan,\n",
    "        \"draft\": draft_answer,\n",
    "        \"critique\": critique,\n",
    "        \"final\": final_answer,\n",
    "        \"final_result_obj\": final_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a9f0a",
   "metadata": {},
   "source": [
    "### Stream Final Answer (run_stream)\n",
    "This version of AutoGen only supports streaming without `recipients=`, \n",
    "we stream the final answer to the user, NOT between agents,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8658238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream final answer to user \n",
    "async def stream_final_answer(query: str):\n",
    "    result = await multi_agent_rag(query)\n",
    "\n",
    "    print(\"\\n\\n=== FINAL ANSER (streamed) ==\\n\")\n",
    "\n",
    "    async for frame in writer.run_stream(task=result[\"final\"]):\n",
    "        # Some frames may not have content (i.e. start/end events)1\n",
    "        if hasattr(frame, 'content') and frame.content:\n",
    "            print(frame.content, end=\"\", flush=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11388c38",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a07e98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AutoGen is a framework for building multi-agent systems that can collaborate to solve complex tasks. It supports tools, function calling, and orchestration patterns for LLM-based agents. Retrieval-Augmented Generation (RAG) is a technique where a model retrieves relevant context from an', 'a technique where a model retrieves relevant context from an external knowledge base (like a vector database) and uses it to ground its responses. By combining AutoGen with a vector store like ChromaDB, you can build multi-agent systems that retrieve,', 'store like ChromaDB, you can build multi-agent systems that retrieve, reason, and respond with up-to-date, domain-specific knowledge.']\n",
      "['chunk-0', 'chunk-1', 'chunk-2']\n",
      "\n",
      "\n",
      "=== FINAL ANSER (streamed) ==\n",
      "\n",
      "AutoGen is a framework specifically designed for building multi-agent systems that can effectively collaborate to solve complex tasks. It supports the integration of tools, function calling, and orchestration patterns, which are crucial for agents utilizing large language models (LLMs). These features enhance the capability of the agents to work together efficiently, addressing challenges that might be insurmountable for a single agent.\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a technique that enhances language model outputs by retrieving relevant context from external knowledge bases, such as vector databases. By incorporating this contextual information, RAG ensures that the models’ responses are grounded in accurate and up-to-date facts, thereby improving both the relevance and precision of the outputs.\n",
      "\n",
      "When AutoGen and RAG are integrated, particularly with a vector store like ChromaDB, the synergy is striking. A vector store, such as ChromaDB, is a specialized database that efficiently manages and retrieves high-dimensional vector data, enabling these systems to retrieve, reason with, and respond using domain-specific and up-to-date knowledge. This integration allows multi-agent systems not only to collaborate effectively but also to leverage advanced contextual retrieval to perform more intelligently in dynamic environments.\n",
      "\n",
      "For practical applications, this integration is impactful in numerous areas. For example, intelligent virtual assistants can deliver more accurate, context-aware responses, significantly enhancing user interactions. In customer support systems, this setup allows for personalized assistance, utilizing real-time data to improve customer satisfaction. In fields such as finance or healthcare, where precision and timely data are paramount, these systems can assist in strategic decision-making by navigating and interpreting complex information landscapes with accuracy and confidence.\n",
      "\n",
      "In conclusion, the integration of AutoGen with RAG, especially when augmented with vector stores like ChromaDB, substantially enhances the capabilities of multi-agent systems to collaborate and deliver accurate, context-rich responses. This synergy not only improves decision-making and task execution in current dynamic environments but also positions these systems for future advancements, potentially transforming how industries leverage AI to address complex challenges efficiently and effectively.AutoGen is a framework meticulously designed to construct multi-agent systems that can collaboratively solve complex tasks. It provides essential functionalities like tool integration, function calling, and orchestration patterns, especially for agents that utilize large language models (LLMs). These features significantly enhance the ability of agents to interact and work together efficiently, addressing problems that might be too challenging for a single agent to handle on its own.\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a technique that significantly improves the outputs of language models by retrieving relevant context from external knowledge bases, such as vector databases. This approach ensures that model responses are grounded in precise and current information, thereby improving the relevance and accuracy of the outputs.\n",
      "\n",
      "When AutoGen is integrated with RAG, particularly in conjunction with vector stores like ChromaDB, the resulting synergy is substantial. A vector store, such as ChromaDB, is a specialized database designed to store and manage high-dimensional vector data efficiently. This allows systems to retrieve, reason with, and respond using domain-specific, up-to-date knowledge. This integration empowers multi-agent systems to not only collaborate effectively but also leverage advanced contextual retrieval, enhancing their performance in dynamic environments.\n",
      "\n",
      "This combination has practical applications across various areas. In intelligent virtual assistants, for example, the integration enables the delivery of more accurate, context-aware responses, significantly enhancing user interaction. In customer support systems, it allows for personalized assistance using real-time data, boosting customer satisfaction. Additionally, in fields like finance and healthcare, where timely and precise data is critical, these systems can support strategic decision-making by effectively navigating and interpreting complex informational landscapes.\n",
      "\n",
      "In summary, the integration of AutoGen and RAG, particularly when bolstered by vector stores like ChromaDB, greatly enhances the capabilities of multi-agent systems to collaborate and deliver accurate, context-rich responses. This powerful synergy not only optimizes decision-making and task execution in today's dynamic environments but also sets the stage for future advancements, potentially transforming how industries employ AI to tackle complex challenges efficiently and effectively."
     ]
    }
   ],
   "source": [
    "result = await stream_final_answer(\"What is AutoGen and how does it relate to RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "163a818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGen is a framework specifically designed for building multi-agent systems that can effectively collaborate to solve complex tasks. It supports the integration of tools, function calling, and orchestration patterns, which are crucial for agents utilizing large language models (LLMs). These features enhance the capability of the agents to work together efficiently, addressing challenges that might be insurmountable for a single agent.\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a technique that enhances language model outputs by retrieving relevant context from external knowledge bases, such as vector databases. By incorporating this contextual information, RAG ensures that the models’ responses are grounded in accurate and up-to-date facts, thereby improving both the relevance and precision of the outputs.\n",
      "\n",
      "When AutoGen and RAG are integrated, particularly with a vector store like ChromaDB, the synergy is striking. A vector store, such as ChromaDB, is a specialized database that efficiently manages and retrieves high-dimensional vector data, enabling these systems to retrieve, reason with, and respond using domain-specific and up-to-date knowledge. This integration allows multi-agent systems not only to collaborate effectively but also to leverage advanced contextual retrieval to perform more intelligently in dynamic environments.\n",
      "\n",
      "For practical applications, this integration is impactful in numerous areas. For example, intelligent virtual assistants can deliver more accurate, context-aware responses, significantly enhancing user interactions. In customer support systems, this setup allows for personalized assistance, utilizing real-time data to improve customer satisfaction. In fields such as finance or healthcare, where precision and timely data are paramount, these systems can assist in strategic decision-making by navigating and interpreting complex information landscapes with accuracy and confidence.\n",
      "\n",
      "In conclusion, the integration of AutoGen with RAG, especially when augmented with vector stores like ChromaDB, substantially enhances the capabilities of multi-agent systems to collaborate and deliver accurate, context-rich responses. This synergy not only improves decision-making and task execution in current dynamic environments but also positions these systems for future advancements, potentially transforming how industries leverage AI to address complex challenges efficiently and effectively.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: result in format of user-defined dict (declared above)\n",
    "print(result[\"final_result_obj\"].messages[-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
